{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIXv5GcknjF65Eow/I4LJ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harini1703/Natural-Language-Processing-Pratice/blob/main/NLP_Tokenization_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USxHOfsawk7z",
        "outputId": "c9b11aa7-5a91-418a-affd-64872f4178a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " corpus=\"\"\"Hello welcome ,to harini's home .\n",
        " I am doing an internship in C-DAC Mohali ,where i have to learn and work on projects.\"\"\""
      ],
      "metadata": {
        "id": "rO6yWCvdwu_a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "i5a7XVEFw4tR",
        "outputId": "a707542c-21c5-42a2-d774-246a30729a29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello welcome ,to harini's home .\\nI am doing an internship in C-DAC Mohali ,where i have to learn and work on projects.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwL28kLNw64j",
        "outputId": "ce48c38b-9c91-4010-a4ba-90134eedd267"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello welcome ,to harini's home .\n",
            "I am doing an internship in C-DAC Mohali ,where i have to learn and work on projects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download ('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo-48TSCxIRu",
        "outputId": "1d55586c-83cd-4b24-b8d0-5c4aef788c7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize"
      ],
      "metadata": {
        "id": "F5JowN3lw81e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents=sent_tokenize(corpus) #Converting sentence to paragraph"
      ],
      "metadata": {
        "id": "5i9_EADdxD2e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHK7tLhszMu3",
        "outputId": "7d529b4e-f2ed-4feb-eaf6-d900f0b0d88d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Hello welcome ,to harini's home .\",\n",
              " 'I am doing an internship in C-DAC Mohali ,where i have to learn and work on projects.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "C1F4bLiUxTdr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kx0APfCzDUz",
        "outputId": "6ac2eb58-12ca-40e6-f5cd-56004dc63c71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'harini',\n",
              " \"'s\",\n",
              " 'home',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'doing',\n",
              " 'an',\n",
              " 'internship',\n",
              " 'in',\n",
              " 'C-DAC',\n",
              " 'Mohali',\n",
              " ',',\n",
              " 'where',\n",
              " 'i',\n",
              " 'have',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'and',\n",
              " 'work',\n",
              " 'on',\n",
              " 'projects',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#will throw an error- word_tokenize(documents)\n",
        "\n",
        "for sentences in documents:\n",
        "  print(word_tokenize(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9kQE_nJzS4f",
        "outputId": "4c9bf2f0-a896-4267-8551-d039e4f20e32"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'welcome', ',', 'to', 'harini', \"'s\", 'home', '.']\n",
            "['I', 'am', 'doing', 'an', 'internship', 'in', 'C-DAC', 'Mohali', ',', 'where', 'i', 'have', 'to', 'learn', 'and', 'work', 'on', 'projects', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "WoSROoAsTK1z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus) #Remove apphotphi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbfngSLGUJZZ",
        "outputId": "c6c732f3-0133-4e47-bb49-6ff130e71f3a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'harini',\n",
              " \"'\",\n",
              " 's',\n",
              " 'home',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'doing',\n",
              " 'an',\n",
              " 'internship',\n",
              " 'in',\n",
              " 'C',\n",
              " '-',\n",
              " 'DAC',\n",
              " 'Mohali',\n",
              " ',',\n",
              " 'where',\n",
              " 'i',\n",
              " 'have',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'and',\n",
              " 'work',\n",
              " 'on',\n",
              " 'projects',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tree bank word tokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "HTIygcsUUMH2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "8Wmm8TAVUc7z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlZTtSQ8UkGB",
        "outputId": "a15348ee-4ae2-4a21-df40-0965aaef222b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'harini',\n",
              " \"'s\",\n",
              " 'home',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'doing',\n",
              " 'an',\n",
              " 'internship',\n",
              " 'in',\n",
              " 'C-DAC',\n",
              " 'Mohali',\n",
              " ',',\n",
              " 'where',\n",
              " 'i',\n",
              " 'have',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'and',\n",
              " 'work',\n",
              " 'on',\n",
              " 'projects',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test=['Hello welcome ,to harini home '\n",
        "     'I am doing an internship in C-DAC Mohali ,where i have to learn and work on projects.']"
      ],
      "metadata": {
        "id": "3tT61KsWUwLA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "1x8YR-wDcJPu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer(num_words=100)#50 frequen words\n",
        "tokenizer.fit_on_texts(test)\n",
        "word_index = tokenizer.word_index\n",
        "for word, index in word_index.items():\n",
        "    print(f'{word}: {index}')"
      ],
      "metadata": {
        "id": "yCzsQZm8cVaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbB3D3VZdrve",
        "outputId": "159763f9-e15e-429d-a9ff-5413faa5e17d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://neptune.ai/blog/tokenization-in-nlp"
      ],
      "metadata": {
        "id": "qV7NEuDsfTTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test2=['''Bali is predominantly a Hindu country. Bali is known for its elaborate, traditional dancing.\n",
        "The dancing is inspired by its Hindi beliefs. Most of the dancing portrays tales of good versus evil.\n",
        "       To watch the dancing is a breathtaking experience. Lombok has some impressive points of interest – the majestic Gunung Rinjani is an active volcano.\n",
        "       It is the second highest peak in Indonesia. Art is a Balinese passion. Batik paintings and carved statues make popular souvenirs.\n",
        "       Artists can be seen whittling and painting on the streets, particularly in Ubud. It is easy to appreciate each island as an attractive tourist destination.\n",
        "       Majestic scenery; rich culture; white sands and warm, azure waters draw visitors like magnets every year. Snorkelling and diving around the nearby Gili Islands is magnificent. Marine fish, starfish, turtles and coral reef are present in abundance.\n",
        "       Bali and Lombok are part of the Indonesian archipelago.\n",
        "       Bali has some spectacular temples. The most significant is the Mother Temple, Besakih. The inhabitants of Lombok are mostly Muslim with a Hindu minority.\n",
        "       Lombok remains the most understated of the two islands.\n",
        "Lombok has several temples worthy of a visit, though they are less prolific. Bali and Lombok are neighbouring islands.''']"
      ],
      "metadata": {
        "id": "ki8W5hv7fUhv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2"
      ],
      "metadata": {
        "id": "JRJYZMg3g3rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1=Tokenizer(num_words=100)\n",
        "tokenizer1.fit_on_texts(test2)"
      ],
      "metadata": {
        "id": "vWwHycWog4tC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index1=tokenizer1.word_index\n",
        "for word, index in word_index1.items():\n",
        "    print(f'{word}: {index}')"
      ],
      "metadata": {
        "id": "0YYZp2UDhOmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXT PROCESSING:\n",
        "The first thing you need to do in any NLP project is text preprocessing. Preprocessing input text simply means putting the data into a predictable and analyzable form. It’s a crucial step for building an amazing NLP application.\n",
        "\n",
        "There are different ways to preprocess text:\n",
        "\n",
        "stop word removal,\n",
        "\n",
        "\n",
        "\n",
        "1.   Tokenization\n",
        "\n",
        "1.   Stemming\n",
        "2.   Stop word Removal\n",
        "\n",
        "\n",
        "Among these, the most important step is tokenization. It’s the process of breaking a stream of textual data into words, terms, sentences, symbols, or some other meaningful elements called tokens.\n",
        "\n",
        " A lot of open-source tools are available to perform the tokenization process.\n",
        "\n",
        " **Why do we need tokenization?**\n",
        "\n",
        "Tokenization is the first step in any NLP pipeline. It has an important effect on the rest of your pipeline. A tokenizer breaks unstructured data and natural language text into chunks of information that can be considered as discrete elements. The token occurrences in a document can be used directly as a vector representing that document.\n",
        "\n",
        "This immediately turns an unstructured string (text document) into a numerical data structure suitable for machine learning. They can also be used directly by a computer to trigger useful actions and responses. Or they might be used in a machine learning pipeline as features that trigger more complex decisions or behavior.\n",
        "\n",
        "Tokenization can separate sentences, words, characters, or subwords. When we split the text into sentences, we call it sentence tokenization. For words, we call it word tokenization.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  from nltk import sent_tokenize\n",
        "  from nltk import word_tokenize\n",
        "  sent_tokenize(corpus)\n",
        "  word_tokenize(corpus)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rdl7UNAvZmWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Tokenization with word_index\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "tokenizer=Tokenizer(num_words=100)#50 frequen words\n",
        "tokenizer.fit_on_texts(test)\n",
        "word_index = tokenizer.word_index\n",
        "for word, index in word_index.items():\n",
        "    print(f'{word}: {index}')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nRE0NuTldHS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.White space Tokenization\n",
        "\n",
        "The simplest way to tokenize text is to use whitespace within a string as the “delimiter” of words. This can be accomplished with Python’s split function, which is available on all string object instances as well as on the string built-in class itself. You can change the separator any way you need.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GHueDewjdklB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test3='''Bali is predominantly a Hindu country. Bali is known for its elaborate, traditional dancing.\n",
        "The dancing is inspired by its Hindi beliefs. Most of the dancing portrays tales of good versus evil.\n",
        "       To watch the dancing is a breathtaking experience. Lombok has some impressive points of interest – the majestic Gunung Rinjani is an active volcano.\n",
        "       It is the second highest peak in Indonesia. Art is a Balinese passion. Batik paintings and carved statues make popular souvenirs.\n",
        "       Artists can be seen whittling and painting on the streets, particularly in Ubud. It is easy to appreciate each island as an attractive tourist destination.\n",
        "       Majestic scenery; rich culture; white sands and warm, azure waters draw visitors like magnets every year. Snorkelling and diving around the nearby Gili Islands is magnificent. Marine fish, starfish, turtles and coral reef are present in abundance.\n",
        "       Bali and Lombok are part of the Indonesian archipelago.\n",
        "       Bali has some spectacular temples. The most significant is the Mother Temple, Besakih. The inhabitants of Lombok are mostly Muslim with a Hindu minority.\n",
        "       Lombok remains the most understated of the two islands.\n",
        "Lombok has several temples worthy of a visit, though they are less prolific. Bali and Lombok are neighbouring islands.'''"
      ],
      "metadata": {
        "id": "oJCiMygThg1Q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "-fDsqQ52dzVe",
        "outputId": "eca72c0b-9d72-4c5e-a32c-69347d774872"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bali is predominantly a Hindu country. Bali is known for its elaborate, traditional dancing.\\nThe dancing is inspired by its Hindi beliefs. Most of the dancing portrays tales of good versus evil.\\n       To watch the dancing is a breathtaking experience. Lombok has some impressive points of interest – the majestic Gunung Rinjani is an active volcano.\\n       It is the second highest peak in Indonesia. Art is a Balinese passion. Batik paintings and carved statues make popular souvenirs.\\n       Artists can be seen whittling and painting on the streets, particularly in Ubud. It is easy to appreciate each island as an attractive tourist destination.\\n       Majestic scenery; rich culture; white sands and warm, azure waters draw visitors like magnets every year. Snorkelling and diving around the nearby Gili Islands is magnificent. Marine fish, starfish, turtles and coral reef are present in abundance.\\n       Bali and Lombok are part of the Indonesian archipelago.\\n       Bali has some spectacular temples. The most significant is the Mother Temple, Besakih. The inhabitants of Lombok are mostly Muslim with a Hindu minority.\\n       Lombok remains the most understated of the two islands.\\nLombok has several temples worthy of a visit, though they are less prolific. Bali and Lombok are neighbouring islands.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test3.split(',') #Separates sentences before ','"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkH7tTsoeBsF",
        "outputId": "9579bce2-a576-427f-f4e7-d66d8cd4f65b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bali is predominantly a Hindu country. Bali is known for its elaborate',\n",
              " ' traditional dancing.\\nThe dancing is inspired by its Hindi beliefs. Most of the dancing portrays tales of good versus evil.\\n       To watch the dancing is a breathtaking experience. Lombok has some impressive points of interest – the majestic Gunung Rinjani is an active volcano.\\n       It is the second highest peak in Indonesia. Art is a Balinese passion. Batik paintings and carved statues make popular souvenirs.\\n       Artists can be seen whittling and painting on the streets',\n",
              " ' particularly in Ubud. It is easy to appreciate each island as an attractive tourist destination.\\n       Majestic scenery; rich culture; white sands and warm',\n",
              " ' azure waters draw visitors like magnets every year. Snorkelling and diving around the nearby Gili Islands is magnificent. Marine fish',\n",
              " ' starfish',\n",
              " ' turtles and coral reef are present in abundance.\\n       Bali and Lombok are part of the Indonesian archipelago.\\n       Bali has some spectacular temples. The most significant is the Mother Temple',\n",
              " ' Besakih. The inhabitants of Lombok are mostly Muslim with a Hindu minority.\\n       Lombok remains the most understated of the two islands.\\nLombok has several temples worthy of a visit',\n",
              " ' though they are less prolific. Bali and Lombok are neighbouring islands.']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test3.split() #Separates sentences with ' '"
      ],
      "metadata": {
        "id": "ECeBwolFeEDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.NLTK Word Tokenize\n",
        "\n",
        "\n",
        "*  White Space Tokenization\n",
        "*  NLTK Tokenzie\n",
        "\n",
        "*   Word and Sentence Tokenizer\n",
        "*   Punctuation based Tokenizer\n",
        "\n",
        "\n",
        "*   TreebankWordTokenizer - This tokenizer incorporates a variety of common rules for english word tokenization. It separates phrase-terminating punctuation like (?!.;,) from adjacent tokens and retains decimal numbers as a single token. Besides, it contains rules for English contractions.\n",
        "\n",
        "For example “don’t” is tokenized as [“do”, “n’t”]. You can find all the rules for the Treebank Tokenizer at this link. https://www.nltk.org/api/nltk.tokenize.treebank.html\n",
        "*   Tweet Tokenizer - When we want to apply tokenization in text data like tweets, the tokenizers mentioned above can’t produce practical tokens. Through this issue, NLTK has a rule based tokenizer special for tweets. We can split emojis into different words if we need them for tasks like sentiment analysis.\n",
        "\n",
        "\n",
        "*   MWET tokenizer - NLTK’s multi-word expression tokenizer (MWETokenizer) provides a function add_mwe() that allows the user to enter multiple word expressions before using the tokenizer on the text. More simply, it can merge multi-word expressions into single tokens.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RQLz0Itafbv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize, TreebankWordTokenizer, wordpunct_tokenize, TweetTokenizer, MWETokenizer\n"
      ],
      "metadata": {
        "id": "WfhEf1abekZ0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(test3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yokGjTNmf4XD",
        "outputId": "00b6e533-83fd-464e-a1fa-92934adf6edc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bali is predominantly a Hindu country.',\n",
              " 'Bali is known for its elaborate, traditional dancing.',\n",
              " 'The dancing is inspired by its Hindi beliefs.',\n",
              " 'Most of the dancing portrays tales of good versus evil.',\n",
              " 'To watch the dancing is a breathtaking experience.',\n",
              " 'Lombok has some impressive points of interest – the majestic Gunung Rinjani is an active volcano.',\n",
              " 'It is the second highest peak in Indonesia.',\n",
              " 'Art is a Balinese passion.',\n",
              " 'Batik paintings and carved statues make popular souvenirs.',\n",
              " 'Artists can be seen whittling and painting on the streets, particularly in Ubud.',\n",
              " 'It is easy to appreciate each island as an attractive tourist destination.',\n",
              " 'Majestic scenery; rich culture; white sands and warm, azure waters draw visitors like magnets every year.',\n",
              " 'Snorkelling and diving around the nearby Gili Islands is magnificent.',\n",
              " 'Marine fish, starfish, turtles and coral reef are present in abundance.',\n",
              " 'Bali and Lombok are part of the Indonesian archipelago.',\n",
              " 'Bali has some spectacular temples.',\n",
              " 'The most significant is the Mother Temple, Besakih.',\n",
              " 'The inhabitants of Lombok are mostly Muslim with a Hindu minority.',\n",
              " 'Lombok remains the most understated of the two islands.',\n",
              " 'Lombok has several temples worthy of a visit, though they are less prolific.',\n",
              " 'Bali and Lombok are neighbouring islands.']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(test3))\n",
        "#N.B: The sent_tokenize uses the pre-trained model from tokenizers/punkt/english.pickle."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA7O8C2vgMEC",
        "outputId": "b3dccddb-24a8-48fe-a040-d13898af5057"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Bali', 'is', 'predominantly', 'a', 'Hindu', 'country', '.', 'Bali', 'is', 'known', 'for', 'its', 'elaborate', ',', 'traditional', 'dancing', '.', 'The', 'dancing', 'is', 'inspired', 'by', 'its', 'Hindi', 'beliefs', '.', 'Most', 'of', 'the', 'dancing', 'portrays', 'tales', 'of', 'good', 'versus', 'evil', '.', 'To', 'watch', 'the', 'dancing', 'is', 'a', 'breathtaking', 'experience', '.', 'Lombok', 'has', 'some', 'impressive', 'points', 'of', 'interest', '–', 'the', 'majestic', 'Gunung', 'Rinjani', 'is', 'an', 'active', 'volcano', '.', 'It', 'is', 'the', 'second', 'highest', 'peak', 'in', 'Indonesia', '.', 'Art', 'is', 'a', 'Balinese', 'passion', '.', 'Batik', 'paintings', 'and', 'carved', 'statues', 'make', 'popular', 'souvenirs', '.', 'Artists', 'can', 'be', 'seen', 'whittling', 'and', 'painting', 'on', 'the', 'streets', ',', 'particularly', 'in', 'Ubud', '.', 'It', 'is', 'easy', 'to', 'appreciate', 'each', 'island', 'as', 'an', 'attractive', 'tourist', 'destination', '.', 'Majestic', 'scenery', ';', 'rich', 'culture', ';', 'white', 'sands', 'and', 'warm', ',', 'azure', 'waters', 'draw', 'visitors', 'like', 'magnets', 'every', 'year', '.', 'Snorkelling', 'and', 'diving', 'around', 'the', 'nearby', 'Gili', 'Islands', 'is', 'magnificent', '.', 'Marine', 'fish', ',', 'starfish', ',', 'turtles', 'and', 'coral', 'reef', 'are', 'present', 'in', 'abundance', '.', 'Bali', 'and', 'Lombok', 'are', 'part', 'of', 'the', 'Indonesian', 'archipelago', '.', 'Bali', 'has', 'some', 'spectacular', 'temples', '.', 'The', 'most', 'significant', 'is', 'the', 'Mother', 'Temple', ',', 'Besakih', '.', 'The', 'inhabitants', 'of', 'Lombok', 'are', 'mostly', 'Muslim', 'with', 'a', 'Hindu', 'minority', '.', 'Lombok', 'remains', 'the', 'most', 'understated', 'of', 'the', 'two', 'islands', '.', 'Lombok', 'has', 'several', 'temples', 'worthy', 'of', 'a', 'visit', ',', 'though', 'they', 'are', 'less', 'prolific', '.', 'Bali', 'and', 'Lombok', 'are', 'neighbouring', 'islands', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test4=\"Hello there! How are you doing today? I'm feeling great, but the weather is quite unpredictable. Don't you think so? Well, let's grab a cup of coffee and catch up!\"\n"
      ],
      "metadata": {
        "id": "IexCST-lgZdx"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(test4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5sRJXVjhHOK",
        "outputId": "6c1048a9-dee6-42b2-e9d3-1ee377903ba4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'there',\n",
              " '!',\n",
              " 'How',\n",
              " 'are',\n",
              " 'you',\n",
              " 'doing',\n",
              " 'today',\n",
              " '?',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'feeling',\n",
              " 'great',\n",
              " ',',\n",
              " 'but',\n",
              " 'the',\n",
              " 'weather',\n",
              " 'is',\n",
              " 'quite',\n",
              " 'unpredictable',\n",
              " '.',\n",
              " 'Do',\n",
              " \"n't\",\n",
              " 'you',\n",
              " 'think',\n",
              " 'so',\n",
              " '?',\n",
              " 'Well',\n",
              " ',',\n",
              " 'let',\n",
              " \"'s\",\n",
              " 'grab',\n",
              " 'a',\n",
              " 'cup',\n",
              " 'of',\n",
              " 'coffee',\n",
              " 'and',\n",
              " 'catch',\n",
              " 'up',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test4.split('?')"
      ],
      "metadata": {
        "id": "VnfryTL1hNJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(test4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEg6e6sahYj_",
        "outputId": "ea018e95-8593-4f31-b09a-2ed8045d0a8a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'there',\n",
              " '!',\n",
              " 'How',\n",
              " 'are',\n",
              " 'you',\n",
              " 'doing',\n",
              " 'today',\n",
              " '?',\n",
              " 'I',\n",
              " \"'\",\n",
              " 'm',\n",
              " 'feeling',\n",
              " 'great',\n",
              " ',',\n",
              " 'but',\n",
              " 'the',\n",
              " 'weather',\n",
              " 'is',\n",
              " 'quite',\n",
              " 'unpredictable',\n",
              " '.',\n",
              " 'Don',\n",
              " \"'\",\n",
              " 't',\n",
              " 'you',\n",
              " 'think',\n",
              " 'so',\n",
              " '?',\n",
              " 'Well',\n",
              " ',',\n",
              " 'let',\n",
              " \"'\",\n",
              " 's',\n",
              " 'grab',\n",
              " 'a',\n",
              " 'cup',\n",
              " 'of',\n",
              " 'coffee',\n",
              " 'and',\n",
              " 'catch',\n",
              " 'up',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(test4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3PMay9Qhfyb",
        "outputId": "8fc056b7-4809-4ba1-cdd9-ba249de81e50"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello there!',\n",
              " 'How are you doing today?',\n",
              " \"I'm feeling great, but the weather is quite unpredictable.\",\n",
              " \"Don't you think so?\",\n",
              " \"Well, let's grab a cup of coffee and catch up!\"]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ent = \"I'm a dog and it's great! You're cool and Sandy's book is big. Don't tell her, you'll regret it! 'Hey', she'll say!\""
      ],
      "metadata": {
        "id": "_IwlGXV6iYFI"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AanSQBLQjpQK",
        "outputId": "a03edd65-afa6-44e5-b9c8-19b474a99690"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm a dog and it's great! You're cool and Sandy's book is big. Don't tell her, you'll regret it! 'Hey', she'll say!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(ent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPCnDGF8jrmX",
        "outputId": "367d6343-b0c7-4ad7-8ad9-b9a6b53008c1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'m\", 'a', 'dog', 'and', 'it', \"'s\", 'great', '!', 'You', \"'re\", 'cool', 'and', 'Sandy', \"'s\", 'book', 'is', 'big', '.', 'Do', \"n't\", 'tell', 'her', ',', 'you', \"'ll\", 'regret', 'it', '!', \"'Hey\", \"'\", ',', 'she', \"'ll\", 'say', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/50240029/nltk-wordpunct-tokenize-vs-word-tokenize"
      ],
      "metadata": {
        "id": "J7OZTdL2lFu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordpunct_tokenize(ent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8SZz_LqjzUm",
        "outputId": "b17a99b3-3922-4309-981e-6dbe39dbccd5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'\", 'm', 'a', 'dog', 'and', 'it', \"'\", 's', 'great', '!', 'You', \"'\", 're', 'cool', 'and', 'Sandy', \"'\", 's', 'book', 'is', 'big', '.', 'Don', \"'\", 't', 'tell', 'her', ',', 'you', \"'\", 'll', 'regret', 'it', '!', \"'\", 'Hey', \"',\", 'she', \"'\", 'll', 'say', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test6=\"What you don't want to be done to yourself , don't do to others... \""
      ],
      "metadata": {
        "id": "ZQUX_XEUj74Z"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TreebankWordTokenizer()\n",
        "print(tokenizer.tokenize(test6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QPHANXkmKae",
        "outputId": "6d0af1b1-71af-4a12-e9a6-5c8bebc96c34"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', 'you', 'do', \"n't\", 'want', 'to', 'be', 'done', 'to', 'yourself', ',', 'do', \"n't\", 'do', 'to', 'others', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet=\"Just finished a fantastic book! 📚 Highly recommend it to all book lovers. Can't wait to discuss it with my friends at the book club tonight! 📖🍵 #BookClub #ReadingIsFun\"\n"
      ],
      "metadata": {
        "id": "FVO1nEs2mfDr"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rKBhoHYTnll4",
        "outputId": "60f518b1-a3d9-4327-c554-aea22c69b1f7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Just finished a fantastic book! 📚 Highly recommend it to all book lovers. Can't wait to discuss it with my friends at the book club tonight! 📖🍵 #BookClub #ReadingIsFun\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizertweet=TweetTokenizer()\n",
        "print(tokenizertweet.tokenize(tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAL5vPhonnSo",
        "outputId": "7d6f474a-6c9e-4dd9-d44c-850cf9929845"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Just', 'finished', 'a', 'fantastic', 'book', '!', '📚', 'Highly', 'recommend', 'it', 'to', 'all', 'book', 'lovers', '.', \"Can't\", 'wait', 'to', 'discuss', 'it', 'with', 'my', 'friends', 'at', 'the', 'book', 'club', 'tonight', '!', '📖', '🍵', '#BookClub', '#ReadingIsFun']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MWET tokenizer"
      ],
      "metadata": {
        "id": "OAOxwfHRoAq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test10=\"Hope , is the only thing stronger than fear ! Hunger Games #Hope\""
      ],
      "metadata": {
        "id": "nOUON5Jmny3n"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer3=MWETokenizer()\n",
        "print(tokenizer3.tokenize(word_tokenize(test10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2-HaVFKpROG",
        "outputId": "5c598880-bc06-41c7-88d2-03442a065116"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger', 'Games', '#', 'Hope']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test10=\"Hope , is the only thing stronger than fear ! Hunger Games #Hope\"\n",
        "tokenizer3=MWETokenizer()\n",
        "tokenizer3.add_mwe((\"Hunger\",'Games'))\n",
        "print(tokenizer3.tokenize(word_tokenize(test10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNuHT2Dhpmn-",
        "outputId": "b0e265a6-e015-4078-9916-3182b60cd0ea"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger_Games', '#', 'Hope']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextBlob Word Tokenize\n",
        "\n",
        "TextBlob is a Python library for processing textual data. It provides a consistent API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
        "\n",
        "We could notice that the TextBlob tokenizer removes the punctuations. In addition, it has rules for English contractions.\n"
      ],
      "metadata": {
        "id": "2kUIfmQ5rv5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U textblob\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwz3HOenp-L1",
        "outputId": "a9edbdbe-cbaa-495c-f55c-2dec3fcab308"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOa5rZ5gqY4F",
        "outputId": "09ef22e9-e089-4800-ceb4-37aa4c08278f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "test11=\"Hope , is the only thing stronger than fear ! Hunger Games #Hope\"\n",
        "\n"
      ],
      "metadata": {
        "id": "q2LXsOaqqgXV"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob_object=TextBlob(test11)"
      ],
      "metadata": {
        "id": "wMK0akSHq1iY"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_words=blob_object.words"
      ],
      "metadata": {
        "id": "d2eoznhXrB07"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpqdPZaUrI1F",
        "outputId": "f0c07e56-71d2-4d87-fcf1-b7404199427e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hope', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', 'Hunger', 'Games', 'Hope']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5IZn-TlrSZI",
        "outputId": "7e093bb9-614e-4db9-e647-9ec68ed379d9"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCy Tokenizer\n",
        "\n",
        "SpaCy is an open-source Python library that parses and understands large volumes of text. With available models catering to specific languages (English, French, German, etc.), it handles NLP tasks with the most efficient implementation of common algorithms.\n",
        "\n",
        "spaCy tokenizer provides the flexibility to specify special tokens that don’t need to be segmented, or need to be segmented using special rules for each language, for example punctuation at the end of a sentence should be split off – whereas “U.K.” should remain one token."
      ],
      "metadata": {
        "id": "_6EoC2r5sKdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJByeQUSrWyL",
        "outputId": "d5e6c2c7-47f5-4b11-b929-fafd7a14d092"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXXYHc13tDA_",
        "outputId": "014bc9e8-a963-42b4-d59b-7ba8159dd81c"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-29 06:11:53.038600: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-29 06:11:53.038678: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-29 06:11:53.038725: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-29 06:11:54.241856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## spaCy Tokenizer"
      ],
      "metadata": {
        "id": "OzH7QXgVuBFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "PdEWsv_3tHJE",
        "outputId": "6824a9d4-785f-4ce8-e51e-53e96f1e0ede"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bali is predominantly a Hindu country. Bali is known for its elaborate, traditional dancing.\\nThe dancing is inspired by its Hindi beliefs. Most of the dancing portrays tales of good versus evil.\\n       To watch the dancing is a breathtaking experience. Lombok has some impressive points of interest – the majestic Gunung Rinjani is an active volcano.\\n       It is the second highest peak in Indonesia. Art is a Balinese passion. Batik paintings and carved statues make popular souvenirs.\\n       Artists can be seen whittling and painting on the streets, particularly in Ubud. It is easy to appreciate each island as an attractive tourist destination.\\n       Majestic scenery; rich culture; white sands and warm, azure waters draw visitors like magnets every year. Snorkelling and diving around the nearby Gili Islands is magnificent. Marine fish, starfish, turtles and coral reef are present in abundance.\\n       Bali and Lombok are part of the Indonesian archipelago.\\n       Bali has some spectacular temples. The most significant is the Mother Temple, Besakih. The inhabitants of Lombok are mostly Muslim with a Hindu minority.\\n       Lombok remains the most understated of the two islands.\\nLombok has several temples worthy of a visit, though they are less prolific. Bali and Lombok are neighbouring islands.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "-lYZ1hNXtOJx"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(test6)"
      ],
      "metadata": {
        "id": "p30edkZQtRfO"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token,token.idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X1Bgmcvt1ap",
        "outputId": "e7e9dcd3-4307-42a0-9717-ef017b1176c2"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What 0\n",
            "you 5\n",
            "do 9\n",
            "n't 11\n",
            "want 15\n",
            "to 20\n",
            "be 23\n",
            "done 26\n",
            "to 31\n",
            "yourself 34\n",
            ", 43\n",
            "do 45\n",
            "n't 47\n",
            "do 51\n",
            "to 54\n",
            "others 57\n",
            "... 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gensim Word Tokenizer\n",
        "\n",
        "Gensim is a Python library for topic modeling, document indexing, and similarity retrieval with large corpora. The target audience is the natural language processing (NLP) and information retrieval (IR) community. It offers utility functions for tokenization."
      ],
      "metadata": {
        "id": "E3cMnDw7uRtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim .utils import tokenize\n",
        "list(tokenize(test6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhxzyKitt6bH",
        "outputId": "5afb433f-0173-4cad-d19d-e843366991dc"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What',\n",
              " 'you',\n",
              " 'don',\n",
              " 't',\n",
              " 'want',\n",
              " 'to',\n",
              " 'be',\n",
              " 'done',\n",
              " 'to',\n",
              " 'yourself',\n",
              " 'don',\n",
              " 't',\n",
              " 'do',\n",
              " 'to',\n",
              " 'others']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization with Keras\n",
        "\n",
        "Keras open-source library is one of the most reliable deep learning frameworks. To perform tokenization we use: text_to_word_sequence method from the Class Keras.preprocessing.text class. The great thing about Keras is converting the alphabet in a lower case before tokenizing it, which can be quite a time-saver."
      ],
      "metadata": {
        "id": "7jgadykvukhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "id": "ZbSUN4mgufti"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ntoken=Tokenizer(num_words=200)"
      ],
      "metadata": {
        "id": "pPlwK4Pvu9wk"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OCm8jqS9vDVY",
        "outputId": "aef40e10-7686-4c43-ae90-24061bd38cb2"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What you don't want to be done to yourself , don't do to others... \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ntoken.fit_on_texts(test6)\n",
        "list_words=text_to_word_sequence(test6)\n",
        "print(list_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBSEo0gCvFIE",
        "outputId": "0389999e-79c8-4189-ccb9-44106d3cb52a"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what', 'you', \"don't\", 'want', 'to', 'be', 'done', 'to', 'yourself', \"don't\", 'do', 'to', 'others']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KmJdUhstvT_Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}